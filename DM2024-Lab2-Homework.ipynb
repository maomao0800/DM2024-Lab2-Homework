{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:æ¯›èƒ¤çµœ\n",
    "\n",
    "Student ID:113062596\n",
    "\n",
    "GitHub ID:maomao0800\n",
    "\n",
    "Kaggle name:Maojack\n",
    "\n",
    "Kaggle private scoreboard snapshot:http://localhost:8888/lab/tree/DM2024-Lab2-Homework/img/pic0.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è¼‰è³‡æ–™ä¸¦åˆ†æˆtrainå’Œtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001951D02B640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "identification_df = pd.read_csv(\"data_identification.csv\")\n",
    "all_data = pd.read_json(\"tweets_DM.json\", lines=True, encoding=\"utf-8\")\n",
    "all_data_df = pd.DataFrame(all_data)\n",
    "emotion_df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df['tweet_id'] = all_data_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "all_data_df['hashtags'] = all_data_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "all_data_df['text'] = all_data_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "all_data_df = all_data_df.drop(columns=['_source'])\n",
    "\n",
    "#print(all_data_df['tweet_id'])\n",
    "\n",
    "# å°‡all_data_dfå’Œidentification_dfæ ¹æ“štweet_idåšåˆä½µ\n",
    "merged_df = pd.merge(all_data_df, identification_df, on='tweet_id', how='inner')\n",
    "\n",
    "# åˆ†é›¢å‡ºtrain_dfå’Œtest_df\n",
    "train_df = pd.merge(merged_df[merged_df['identification']=='train'], emotion_df, on='tweet_id', how='inner')\n",
    "train_df = train_df.drop(columns=['identification','_type','_index'])\n",
    "\n",
    "test_df = merged_df[merged_df['identification'] == 'test']\n",
    "test_df = test_df.drop(columns=['identification','_type','_index']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è™•ç†è¡¨æƒ…ç¬¦è™Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(#ç”±GPTç”Ÿæˆ\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# emojiæ¬„ä½å„²å­˜åœ¨textæ¬„ä½ä¸­æ‰¾åˆ°çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "train_df['emojis'] = train_df['text'].apply(lambda x: emoji_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ›¸', 'í¬', 'ç¨®', 'â˜', 'ğ–“', 'å¢ƒ', 'ã€’', 'ğŸ”´', 'âƒ', 'ğŸ†', 'ğŸ˜‡', 'å››', 'ï»‹', 'ğŸ¥¡', 'ä¿¡', 'ğŸ”½', 'æ¯…', 'æŒ™', 'ï¼†', 'ğŸŒ¤', 'ë¶€', 'ğŸ•£', 'è¨€', 'ïº’', 'ğŸŒ¥', 'éš†', 'ğ“ƒ', 'å±±', 'ğ•–', 'ë†']\n"
     ]
    }
   ],
   "source": [
    "# æŠŠæ‰€æœ‰è¡¨æƒ…ç¬¦è™Ÿåˆ—è¡¨å±•å¹³æˆä¸€å€‹å–®ä¸€çš„åˆ—è¡¨\n",
    "all_emojis = list(itertools.chain(*train_df['emojis']))\n",
    "\n",
    "# æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿçµ„åˆï¼Œä¸¦ä¿ç•™å”¯ä¸€çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = [emoji for sublist in all_emojis for emoji in sublist]  # æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿï¼Œå› ç‚ºå¯èƒ½é‚„ä¿æœ‰é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = list(set(unique_emojis))  # å»é™¤é‡è¤‡çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "\n",
    "print(unique_emojis[0:30])# åŒ…å«å“ªäº›ç¬¦è™Ÿæˆ–æ˜¯éŸ“æ–‡ã€ä¸­æ–‡å­—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ†æè¡¨æƒ…ç¬¦è™Ÿè·Ÿemotionçš„é—œä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´ç†å‡ºå…·æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„éƒ¨åˆ†\n",
    "train_df_with_emojis = train_df[train_df['emojis'].apply(len) > 0]\n",
    "train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# æ‹†é–‹æ‰€æœ‰ emoji(ç›¡é‡é¿å…é€£çºŒçš„emoji)\n",
    "all_emojis = list(itertools.chain(*train_df_with_emojis['emojis'].str.split()))\n",
    "\n",
    "# å»ºç«‹ emoji å’Œ emotion çš„çµ„åˆ\n",
    "emoji_emotion_data = []\n",
    "for index, row in train_df_with_emojis.iterrows():\n",
    "    emojis = row['emojis'].split()  # ä»¥ç©ºæ ¼åˆ†å‰²æ¯è¡Œä¸­çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emotion = row['emotion']\n",
    "    for emoji in emojis:\n",
    "        emoji_emotion_data.append([emoji, emotion])\n",
    "\n",
    "emoji_emotion_df = pd.DataFrame(emoji_emotion_data, columns=['emoji', 'emotion'])\n",
    "\n",
    "# çµ±è¨ˆæ¯å€‹emotionæœ‰å¤šå°‘å€‹ç¨ç‰¹çš„ emoji\n",
    "emotion_emoji_counts = emoji_emotion_df.groupby('emotion')['emoji'].nunique().reset_index(name='unique_emoji_count')\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(emotion_emoji_counts)# çœ‹å¾—å‡ºæ˜é¡¯å·®åˆ¥ï¼Œé›–ç„¶dfä¸­å¯èƒ½é‚„æ˜¯åŒ…å«é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿï¼Œä½†æ˜¯æŠŠå®ƒå€‘çœ‹åšä¸€å€‹è¡¨æƒ…ç¬¦è™Ÿé‚è¼¯ä¸Šä¹Ÿå¯ä»¥æ¥å—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotion dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'joy': 1,\n",
       " 'trust': 2,\n",
       " 'fear': 3,\n",
       " 'anticipation': 4,\n",
       " 'disgust': 5,\n",
       " 'sadness': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_to_id = {'anger': 0,\n",
    " 'joy': 1,\n",
    " 'trust': 2,\n",
    " 'fear': 3,\n",
    " 'anticipation': 4,\n",
    " 'disgust': 5,\n",
    " 'sadness': 6,\n",
    " 'surprise': 7}\n",
    "id_to_emotion = {v: k for k, v in emotion_to_id.items()}\n",
    "emotion_to_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFåšwords embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.9, max_features=300,ngram_range=(1, 2))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
    "train_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "                             \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(test_df[\"text\"])\n",
    "test_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "test_tfidf_df = test_tfidf_df.reindex(columns=train_tfidf_df.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ·»åŠ emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†trainæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90973/90973 [00:17<00:00, 5329.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†testæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25749/25749 [00:05<00:00, 4826.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è™•ç†å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ä¾†æå–è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\")\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# è™•ç†æ–‡æœ¬ä¸¦è¨ˆç®—æ¯æ¢æ–‡æœ¬çš„emojiæ•¸é‡\n",
    "def preprocess_and_add_to_df(texts, batch_size):\n",
    "    emoji_count_list = []  # ç”¨ä¾†å„²å­˜æ¯æ¢æ–‡æœ¬ä¸­çš„ emoji æ•¸é‡\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"è™•ç†ä¸­\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # è¨ˆç®—æ¯æ¢æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦è™Ÿæ•¸é‡\n",
    "        batch_emoji_counts = [len(extract_emojis(text)) for text in batch_texts]\n",
    "        emoji_count_list.extend(batch_emoji_counts)\n",
    "\n",
    "    return emoji_count_list\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†trainæ–‡æœ¬...\")\n",
    "train_emoji_counts = preprocess_and_add_to_df(\n",
    "    train_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "train_df[\"emoji_count\"] = train_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†testæ–‡æœ¬...\")\n",
    "test_emoji_counts = preprocess_and_add_to_df(\n",
    "    test_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "test_df[\"emoji_count\"] = test_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "print(\"è™•ç†å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆä½µTF-IDFå’Œemoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡ emoji_count æ¬„ä½èˆ‡ TF-IDF ç‰¹å¾µåˆä½µ\n",
    "train_features = pd.concat([train_tfidf_df, train_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "test_features = pd.concat([test_tfidf_df, test_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# å‡è¨­ä½ çš„æ¨™ç±¤æ˜¯ \"emotion\" æ¬„ä½\n",
    "train_labels = train_df[\"emotion\"].map(lambda x: emotion_to_id[x]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–éš¨æ©Ÿæ£®æ—æ¨¡å‹\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# é©—è­‰æ¨¡å‹\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-Score: 0.0019\n",
      "Accuracy: 0.0077\n"
     ]
    }
   ],
   "source": [
    "sample_df = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬\n",
    "test_predictions = rf_model.predict(test_features)\n",
    "\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "# ç²å–çœŸå¯¦æ¨™ç±¤å’Œé æ¸¬æ¨™ç±¤\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# è¨“ç·´å›æ­¸æ¨¡å‹\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬ä¸¦è¨ˆç®—å‡æ–¹èª¤å·®\n",
    "test_predictions = regressor.predict(test_features)\n",
    "y_pred_labels = [id_to_emotion[round(pred)] for pred in test_predictions]\n",
    "\n",
    "#æ¯”å°tweet_idä¸¦æ¯”è¼ƒé æ¸¬æ¨™ç±¤åŠçœŸå¯¦æ¨™ç±¤\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMkaggle",
   "language": "python",
   "name": "dmkaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
