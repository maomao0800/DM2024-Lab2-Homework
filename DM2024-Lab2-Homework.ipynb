{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:毛胤絜\n",
    "\n",
    "Student ID:113062596\n",
    "\n",
    "GitHub ID:maomao0800\n",
    "\n",
    "Kaggle name:Maojack\n",
    "\n",
    "Kaggle private scoreboard snapshot:http://localhost:8888/lab/tree/DM2024-Lab2-Homework/img/pic0.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加載資料並分成train和test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001951D02B640>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "identification_df = pd.read_csv(\"data_identification.csv\")\n",
    "all_data = pd.read_json(\"tweets_DM.json\", lines=True, encoding=\"utf-8\")\n",
    "all_data_df = pd.DataFrame(all_data)\n",
    "emotion_df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df['tweet_id'] = all_data_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "all_data_df['hashtags'] = all_data_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "all_data_df['text'] = all_data_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "all_data_df = all_data_df.drop(columns=['_source'])\n",
    "\n",
    "#print(all_data_df['tweet_id'])\n",
    "\n",
    "# 將all_data_df和identification_df根據tweet_id做合併\n",
    "merged_df = pd.merge(all_data_df, identification_df, on='tweet_id', how='inner')\n",
    "\n",
    "# 分離出train_df和test_df\n",
    "train_df = pd.merge(merged_df[merged_df['identification']=='train'], emotion_df, on='tweet_id', how='inner')\n",
    "train_df = train_df.drop(columns=['identification','_type','_index'])\n",
    "\n",
    "test_df = merged_df[merged_df['identification'] == 'test']\n",
    "test_df = test_df.drop(columns=['identification','_type','_index']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 處理表情符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(#由GPT生成\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# emoji欄位儲存在text欄位中找到的表情符號\n",
    "train_df['emojis'] = train_df['text'].apply(lambda x: emoji_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🛸', '크', '種', '☝', '𝖓', '境', '〒', '🔴', '❃', '🏆', '😇', '四', 'ﻋ', '🥡', '信', '🔽', '毅', '挙', '＆', '🌤', '부', '🕣', '言', 'ﺒ', '🌥', '隆', '𝓃', '山', '𝕖', '농']\n"
     ]
    }
   ],
   "source": [
    "# 把所有表情符號列表展平成一個單一的列表\n",
    "all_emojis = list(itertools.chain(*train_df['emojis']))\n",
    "\n",
    "# 拆開表情符號組合，並保留唯一的表情符號\n",
    "unique_emojis = [emoji for sublist in all_emojis for emoji in sublist]  # 拆開表情符號，因為可能還保有連續的表情符號\n",
    "unique_emojis = list(set(unique_emojis))  # 去除重複的表情符號\n",
    "\n",
    "print(unique_emojis[0:30])# 包含哪些符號或是韓文、中文字\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析表情符號跟emotion的關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理出具有表情符號的部分\n",
    "train_df_with_emojis = train_df[train_df['emojis'].apply(len) > 0]\n",
    "train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# 拆開所有 emoji(盡量避免連續的emoji)\n",
    "all_emojis = list(itertools.chain(*train_df_with_emojis['emojis'].str.split()))\n",
    "\n",
    "# 建立 emoji 和 emotion 的組合\n",
    "emoji_emotion_data = []\n",
    "for index, row in train_df_with_emojis.iterrows():\n",
    "    emojis = row['emojis'].split()  # 以空格分割每行中的表情符號\n",
    "    emotion = row['emotion']\n",
    "    for emoji in emojis:\n",
    "        emoji_emotion_data.append([emoji, emotion])\n",
    "\n",
    "emoji_emotion_df = pd.DataFrame(emoji_emotion_data, columns=['emoji', 'emotion'])\n",
    "\n",
    "# 統計每個emotion有多少個獨特的 emoji\n",
    "emotion_emoji_counts = emoji_emotion_df.groupby('emotion')['emoji'].nunique().reset_index(name='unique_emoji_count')\n",
    "\n",
    "# 顯示結果\n",
    "print(emotion_emoji_counts)# 看得出明顯差別，雖然df中可能還是包含連續的表情符號，但是把它們看做一個表情符號邏輯上也可以接受\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotion dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'joy': 1,\n",
       " 'trust': 2,\n",
       " 'fear': 3,\n",
       " 'anticipation': 4,\n",
       " 'disgust': 5,\n",
       " 'sadness': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_to_id = {'anger': 0,\n",
    " 'joy': 1,\n",
    " 'trust': 2,\n",
    " 'fear': 3,\n",
    " 'anticipation': 4,\n",
    " 'disgust': 5,\n",
    " 'sadness': 6,\n",
    " 'surprise': 7}\n",
    "id_to_emotion = {v: k for k, v in emotion_to_id.items()}\n",
    "emotion_to_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF做words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.9, max_features=300,ngram_range=(1, 2))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
    "train_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "                             \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(test_df[\"text\"])\n",
    "test_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "test_tfidf_df = test_tfidf_df.reindex(columns=train_tfidf_df.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理train文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理中: 100%|██████████████████████████████████████████████████████████████████| 90973/90973 [00:17<00:00, 5329.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理test文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理中: 100%|██████████████████████████████████████████████████████████████████| 25749/25749 [00:05<00:00, 4826.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    # 使用正則表達式來提取表情符號\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\")\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# 處理文本並計算每條文本的emoji數量\n",
    "def preprocess_and_add_to_df(texts, batch_size):\n",
    "    emoji_count_list = []  # 用來儲存每條文本中的 emoji 數量\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"處理中\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # 計算每條文本中的表情符號數量\n",
    "        batch_emoji_counts = [len(extract_emojis(text)) for text in batch_texts]\n",
    "        emoji_count_list.extend(batch_emoji_counts)\n",
    "\n",
    "    return emoji_count_list\n",
    "\n",
    "# 只處理 emoji_count\n",
    "print(\"正在處理train文本...\")\n",
    "train_emoji_counts = preprocess_and_add_to_df(\n",
    "    train_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "train_df[\"emoji_count\"] = train_emoji_counts  # 添加emoji數量\n",
    "\n",
    "# 只處理 emoji_count\n",
    "print(\"正在處理test文本...\")\n",
    "test_emoji_counts = preprocess_and_add_to_df(\n",
    "    test_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "test_df[\"emoji_count\"] = test_emoji_counts  # 添加emoji數量\n",
    "\n",
    "print(\"處理完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合併TF-IDF和emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 emoji_count 欄位與 TF-IDF 特徵合併\n",
    "train_features = pd.concat([train_tfidf_df, train_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "test_features = pd.concat([test_tfidf_df, test_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 假設你的標籤是 \"emotion\" 欄位\n",
    "train_labels = train_df[\"emotion\"].map(lambda x: emotion_to_id[x]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分訓練集和驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# 初始化隨機森林模型\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 驗證模型\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-Score: 0.0019\n",
      "Accuracy: 0.0077\n"
     ]
    }
   ],
   "source": [
    "sample_df = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "# 使用訓練好的模型進行預測\n",
    "test_predictions = rf_model.predict(test_features)\n",
    "\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "# 獲取真實標籤和預測標籤\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分訓練集和驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# 訓練回歸模型\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# 預測並計算均方誤差\n",
    "test_predictions = regressor.predict(test_features)\n",
    "y_pred_labels = [id_to_emotion[round(pred)] for pred in test_predictions]\n",
    "\n",
    "#比對tweet_id並比較預測標籤及真實標籤\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMkaggle",
   "language": "python",
   "name": "dmkaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
