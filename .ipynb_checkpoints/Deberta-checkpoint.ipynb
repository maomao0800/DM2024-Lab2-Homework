{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:毛胤絜\n",
    "\n",
    "Student ID:113062596\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加載資料並分成train和test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identification_df = pd.read_csv(\"data_identification.csv\")\n",
    "#all_data = pd.read_json(\"tweets_DM.json\", lines=True, encoding=\"utf-8\")\n",
    "#all_data_df = pd.DataFrame(all_data)\n",
    "#emotion_df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data_df['tweet_id'] = all_data_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "#all_data_df['hashtags'] = all_data_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "#all_data_df['text'] = all_data_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "#all_data_df = all_data_df.drop(columns=['_source'])\n",
    "\n",
    "#print(all_data_df['tweet_id'])\n",
    "\n",
    "# 將all_data_df和identification_df根據tweet_id做合併\n",
    "#merged_df = pd.merge(all_data_df, identification_df, on='tweet_id', how='inner')\n",
    "\n",
    "# 分離出train_df和test_df\n",
    "#train_df = pd.merge(merged_df[merged_df['identification']=='train'], emotion_df, on='tweet_id', how='inner')\n",
    "#train_df = train_df.drop(columns=['identification','_type','_index'])\n",
    "\n",
    "\n",
    "\n",
    "#test_df = merged_df[merged_df['identification'] == 'test']\n",
    "#test_df = test_df.drop(columns=['identification','_type','_index']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 處理表情符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(#由GPT生成\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# emoji欄位儲存在text欄位中找到的表情符號\n",
    "train_df['emojis'] = train_df['text'].apply(lambda x: emoji_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有表情符號列表展平成一個單一的列表\n",
    "#all_emojis = list(itertools.chain(*train_df['emojis']))\n",
    "\n",
    "# 拆開表情符號組合，並保留唯一的表情符號\n",
    "#unique_emojis = [emoji for sublist in all_emojis for emoji in sublist]  # 拆開表情符號，因為可能還保有連續的表情符號\n",
    "#unique_emojis = list(set(unique_emojis))  # 去除重複的表情符號\n",
    "\n",
    "#print(unique_emojis)包含哪些符號或是韓文、中文字\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析表情符號跟emotion的關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_32680\\3835693287.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion  unique_emoji_count\n",
      "0         anger                2269\n",
      "1  anticipation               14525\n",
      "2       disgust                5298\n",
      "3          fear                4358\n",
      "4           joy               37767\n",
      "5       sadness                7879\n",
      "6      surprise                3502\n",
      "7         trust               15586\n"
     ]
    }
   ],
   "source": [
    "# 整理出具有表情符號的部分\n",
    "train_df_with_emojis = train_df[train_df['emojis'].apply(len) > 0]\n",
    "train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# 拆開所有 emoji(盡量避免連續的emoji)\n",
    "all_emojis = list(itertools.chain(*train_df_with_emojis['emojis'].str.split()))\n",
    "\n",
    "# 建立 emoji 和 emotion 的組合\n",
    "emoji_emotion_data = []\n",
    "for index, row in train_df_with_emojis.iterrows():\n",
    "    emojis = row['emojis'].split()  # 以空格分割每行中的表情符號\n",
    "    emotion = row['emotion']\n",
    "    for emoji in emojis:\n",
    "        emoji_emotion_data.append([emoji, emotion])\n",
    "\n",
    "emoji_emotion_df = pd.DataFrame(emoji_emotion_data, columns=['emoji', 'emotion'])\n",
    "\n",
    "# 統計每個emotion有多少個獨特的 emoji\n",
    "emotion_emoji_counts = emoji_emotion_df.groupby('emotion')['emoji'].nunique().reset_index(name='unique_emoji_count')\n",
    "\n",
    "# 顯示結果\n",
    "print(emotion_emoji_counts)# 看得出明顯差別，雖然df中可能還是包含連續的表情符號，但是把它們看做一個表情符號邏輯上也可以接受\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用pretrain model來embedding emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'joy': 1,\n",
       " 'trust': 2,\n",
       " 'fear': 3,\n",
       " 'anticipation': 4,\n",
       " 'disgust': 5,\n",
       " 'sadness': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加載情感分類模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# 獲取embdding向量\n",
    "def get_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True) \n",
    "    # 提取倒數第一層隱藏層\n",
    "    return outputs.hidden_states[-1][0][0].numpy()\n",
    "\n",
    "emotions = [\"anticipation\", \"sadness\", \"fear\", \"joy\", \"anger\", \"trust\", \"disgust\", \"surprise\"]\n",
    "\n",
    "# 計算所有情感詞的embedding\n",
    "embeddings = np.array([get_embedding(emotion) for emotion in emotions])\n",
    "\n",
    "similarities = cosine_similarity(embeddings)\n",
    "similarity_sum = similarities.sum(axis=1)\n",
    "\n",
    "# 根據相似度總和進行排序\n",
    "sorted_emotions = [emotion for _, emotion in sorted(zip(similarity_sum, emotions), reverse=True)]\n",
    "\n",
    "# 建立情感字典\n",
    "emotion_to_id = {emotion: i for i, emotion in enumerate(sorted_emotions)}\n",
    "emotion_to_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deberta embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 選擇模型名稱\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# 加載 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 参数\n",
    "sequence_length = 512  \n",
    "batch_size = 16        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理 train 文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理中: 100%|████████████████████████████████████████████████████████████████████| 90973/90973 [18:01<00:00, 84.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在處理 test 文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理中: 100%|████████████████████████████████████████████████████████████████████| 25749/25749 [07:28<00:00, 57.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本處理完成並添加到數據框！\n"
     ]
    }
   ],
   "source": [
    "# 提取文本中的所有表情符號\n",
    "def extract_emojis(text):\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# 處理文本並計算每條文本的emoji數量\n",
    "def preprocess_and_add_to_df(texts, tokenizer, sequence_length, batch_size, device):\n",
    "    input_ids_list = []  # 用來儲存所有的 input_ids\n",
    "    attention_mask_list = []  # 用來儲存所有的 attention_masks\n",
    "    emoji_count_list = []  # 用來儲存每條文本中的 emoji 數量\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"處理中\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # 計算每條文本中的表情符號數量\n",
    "        batch_emoji_counts = [len(extract_emojis(text)) for text in batch_texts]\n",
    "        emoji_count_list.extend(batch_emoji_counts)\n",
    "\n",
    "        # 使用tokenizer進行分詞處理\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=sequence_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        input_ids_list.extend(inputs[\"input_ids\"].cpu().tolist())\n",
    "        attention_mask_list.extend(inputs[\"attention_mask\"].cpu().tolist())\n",
    "\n",
    "    return input_ids_list, attention_mask_list, emoji_count_list\n",
    "\n",
    "# 配置模型和設備\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 處理 train 資料集\n",
    "print(\"正在處理 train 文本...\")\n",
    "train_input_ids, train_attention_masks, train_emoji_counts = preprocess_and_add_to_df(\n",
    "    train_df['text'].tolist(), tokenizer, sequence_length=512, batch_size=16, device=device\n",
    ")\n",
    "train_df[\"input_ids\"] = train_input_ids\n",
    "train_df[\"attention_mask\"] = train_attention_masks\n",
    "train_df[\"emoji_count\"] = train_emoji_counts  # 將每條文本的 emoji 數量添加到 DataFrame 中\n",
    "\n",
    "# 處理 test 資料集\n",
    "print(\"正在處理 test 文本...\")\n",
    "test_input_ids, test_attention_masks, test_emoji_counts = preprocess_and_add_to_df(\n",
    "    test_df['text'].tolist(), tokenizer, sequence_length=512, batch_size=16, device=device\n",
    ")\n",
    "test_df[\"input_ids\"] = test_input_ids\n",
    "test_df[\"attention_mask\"] = test_attention_masks\n",
    "test_df[\"emoji_count\"] = test_emoji_counts  # 將每條文本的 emoji 數量添加到 DataFrame 中\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"文本處理完成並添加到數據框！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练模型并指定分类任务\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 8  # 类别数\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "model.to(device)  # 将模型转移到 GPU 或 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels, emoji_counts):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        self.emoji_counts = emoji_counts  # 添加 emoji_count 属性\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            \"emoji_count\": torch.tensor(self.emoji_counts[idx], dtype=torch.long), \n",
    "        }\n",
    "\n",
    "\n",
    "# 將train_df拆成model_train跟model_test進行訓練\n",
    "model_train_df, model_test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "model_train_df = model_train_df.reset_index(drop=True)\n",
    "model_test_df = model_test_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = EmotionDataset(\n",
    "    input_ids=model_train_df[\"input_ids\"].tolist(),\n",
    "    attention_masks=model_train_df[\"attention_mask\"].tolist(),\n",
    "    labels=model_train_df[\"emotion\"].map(lambda x: emotion_to_id[x]).tolist(),\n",
    "    emoji_counts=model_train_df[\"emoji_count\"].tolist(),  # 传递 emoji_count\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    input_ids=model_test_df[\"input_ids\"].tolist(),\n",
    "    attention_masks=model_test_df[\"attention_mask\"].tolist(),\n",
    "    labels=model_test_df[\"emotion\"].map(lambda x: emotion_to_id[x]).tolist(),\n",
    "    emoji_counts=model_test_df[\"emoji_count\"].tolist(),  # 传递 emoji_count\n",
    ")\n",
    "\n",
    "# 創建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 使用交叉熵损失\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch size: 1164450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training batch size: {len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size + 1, num_labels)  # +1 用來接收 emoji_count\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, emoji_count=None):\n",
    "        # BERT的前向傳播\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # 拼接 emoji_count 特徵\n",
    "        if emoji_count is not None:\n",
    "            pooled_output = torch.cat((pooled_output, emoji_count.unsqueeze(-1).float()), dim=1)  # 将 emoji_count 添加到输出\n",
    "\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        # 計算損失\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else (logits,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                             | 0/582225 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化模型並將其移動到正確的設備（GPU 或 CPU）\n",
    "model = EmotionModel(model_name=model_name, num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 設定訓練的 epoch 次數\n",
    "epochs = 1\n",
    "\n",
    "# 訓練和評估循環\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # 訓練模式\n",
    "    model.train()  # 確保模型處於訓練模式\n",
    "    train_loss = 0\n",
    "    \n",
    "    # 使用 tqdm 包裝 train_loader，顯示訓練進度條\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        emoji_count = batch[\"emoji_count\"].to(device)  # 取出 emoji_count 資料\n",
    "\n",
    "        # 正常訓練過程\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, emoji_count=emoji_count)\n",
    "        loss = outputs[0]  # 獲取損失值\n",
    "\n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 累計訓練損失\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 計算每一輪的平均訓練損失\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # 評估模式\n",
    "    model.eval()  # 確保模型處於評估模式\n",
    "    predictions = []\n",
    "    true_labels = []  # 用來存儲真實標籤\n",
    "\n",
    "    # 使用 tqdm 包裝 test_loader，顯示評估進度條\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            emoji_count = batch[\"emoji_count\"].to(device)  # 取出 emoji_count 資料\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, emoji_count=emoji_count)\n",
    "            logits = outputs[1]  # 獲取 logits\n",
    "\n",
    "            # 獲取預測的類別\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())  # 保存真實標籤\n",
    "\n",
    "    # 確保預測結果與真實標籤長度一致\n",
    "    assert len(predictions) == len(true_labels), \"預測結果與真實標籤長度不一致\"\n",
    "\n",
    "    # 計算準確率或其他評估指標\n",
    "    accuracy = (np.array(predictions) == np.array(true_labels)).mean()\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 訓練和評估結束後清理 GPU 記憶體\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = EmotionDataset(\n",
    "    test_df[\"input_ids\"],\n",
    "    test_df[\"attention_mask\"],\n",
    "    [0] * len(test_df)  # 测试集无标签，可用占位符\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 train_df 到 CSV 文件\n",
    "train_df.to_csv(\"train_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# 保存 test_df 到 CSV 文件\n",
    "test_df.to_csv(\"test_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMkaggle",
   "language": "python",
   "name": "dmkaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
