{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:æ¯›èƒ¤çµœ\n",
    "\n",
    "Student ID:113062596\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è¼‰è³‡æ–™ä¸¦åˆ†æˆtrainå’Œtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identification_df = pd.read_csv(\"data_identification.csv\")\n",
    "#all_data = pd.read_json(\"tweets_DM.json\", lines=True, encoding=\"utf-8\")\n",
    "#all_data_df = pd.DataFrame(all_data)\n",
    "#emotion_df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data_df['tweet_id'] = all_data_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "#all_data_df['hashtags'] = all_data_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "#all_data_df['text'] = all_data_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "#all_data_df = all_data_df.drop(columns=['_source'])\n",
    "\n",
    "#print(all_data_df['tweet_id'])\n",
    "\n",
    "# å°‡all_data_dfå’Œidentification_dfæ ¹æ“štweet_idåšåˆä½µ\n",
    "#merged_df = pd.merge(all_data_df, identification_df, on='tweet_id', how='inner')\n",
    "\n",
    "# åˆ†é›¢å‡ºtrain_dfå’Œtest_df\n",
    "#train_df = pd.merge(merged_df[merged_df['identification']=='train'], emotion_df, on='tweet_id', how='inner')\n",
    "#train_df = train_df.drop(columns=['identification','_type','_index'])\n",
    "\n",
    "\n",
    "\n",
    "#test_df = merged_df[merged_df['identification'] == 'test']\n",
    "#test_df = test_df.drop(columns=['identification','_type','_index']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è™•ç†è¡¨æƒ…ç¬¦è™Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(#ç”±GPTç”Ÿæˆ\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# emojiæ¬„ä½å„²å­˜åœ¨textæ¬„ä½ä¸­æ‰¾åˆ°çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "train_df['emojis'] = train_df['text'].apply(lambda x: emoji_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ë™', 'â•™', 'ğŸ‘£', 'æ ', 'ğŸ£', 'ğŸ¥¨', 'íƒœ', 'ğŸ”¢', 'æˆ‘', 'ç„', 'é–‰', 'â“›', 'ãƒ½', 'æ¡‚', 'ã', 'â—¦', 'ğŸ§€', 'ãƒ“', 'â•±', 'âœ‚', 'ç´€', 'å±…', 'â“’', 'ï¼”', 'ğŸ¯', 'ğŸ¹', 'ï¼‰', 'ğŸ•', 'ğŸ”’', 'ğŸ¤§']\n"
     ]
    }
   ],
   "source": [
    "# æŠŠæ‰€æœ‰è¡¨æƒ…ç¬¦è™Ÿåˆ—è¡¨å±•å¹³æˆä¸€å€‹å–®ä¸€çš„åˆ—è¡¨\n",
    "all_emojis = list(itertools.chain(*train_df['emojis']))\n",
    "\n",
    "# æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿçµ„åˆï¼Œä¸¦ä¿ç•™å”¯ä¸€çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = [emoji for sublist in all_emojis for emoji in sublist]  # æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿï¼Œå› ç‚ºå¯èƒ½é‚„ä¿æœ‰é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = list(set(unique_emojis))  # å»é™¤é‡è¤‡çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "\n",
    "print(unique_emojis[0:30])# åŒ…å«å“ªäº›ç¬¦è™Ÿæˆ–æ˜¯éŸ“æ–‡ã€ä¸­æ–‡å­—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ†æè¡¨æƒ…ç¬¦è™Ÿè·Ÿemotionçš„é—œä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20200\\3835693287.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion  unique_emoji_count\n",
      "0         anger                2269\n",
      "1  anticipation               14525\n",
      "2       disgust                5298\n",
      "3          fear                4358\n",
      "4           joy               37767\n",
      "5       sadness                7879\n",
      "6      surprise                3502\n",
      "7         trust               15586\n"
     ]
    }
   ],
   "source": [
    "# æ•´ç†å‡ºå…·æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„éƒ¨åˆ†\n",
    "train_df_with_emojis = train_df[train_df['emojis'].apply(len) > 0]\n",
    "train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# æ‹†é–‹æ‰€æœ‰ emoji(ç›¡é‡é¿å…é€£çºŒçš„emoji)\n",
    "all_emojis = list(itertools.chain(*train_df_with_emojis['emojis'].str.split()))\n",
    "\n",
    "# å»ºç«‹ emoji å’Œ emotion çš„çµ„åˆ\n",
    "emoji_emotion_data = []\n",
    "for index, row in train_df_with_emojis.iterrows():\n",
    "    emojis = row['emojis'].split()  # ä»¥ç©ºæ ¼åˆ†å‰²æ¯è¡Œä¸­çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emotion = row['emotion']\n",
    "    for emoji in emojis:\n",
    "        emoji_emotion_data.append([emoji, emotion])\n",
    "\n",
    "emoji_emotion_df = pd.DataFrame(emoji_emotion_data, columns=['emoji', 'emotion'])\n",
    "\n",
    "# çµ±è¨ˆæ¯å€‹emotionæœ‰å¤šå°‘å€‹ç¨ç‰¹çš„ emoji\n",
    "emotion_emoji_counts = emoji_emotion_df.groupby('emotion')['emoji'].nunique().reset_index(name='unique_emoji_count')\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(emotion_emoji_counts)# çœ‹å¾—å‡ºæ˜é¡¯å·®åˆ¥ï¼Œé›–ç„¶dfä¸­å¯èƒ½é‚„æ˜¯åŒ…å«é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿï¼Œä½†æ˜¯æŠŠå®ƒå€‘çœ‹åšä¸€å€‹è¡¨æƒ…ç¬¦è™Ÿé‚è¼¯ä¸Šä¹Ÿå¯ä»¥æ¥å—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotion dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fear',\n",
       " 'anticipation',\n",
       " 'surprise',\n",
       " 'anger',\n",
       " 'disgust',\n",
       " 'trust',\n",
       " 'sadness',\n",
       " 'joy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è¼‰æƒ…æ„Ÿåˆ†é¡æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# ç²å–embddingå‘é‡\n",
    "def get_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True) \n",
    "    # æå–å€’æ•¸ç¬¬ä¸€å±¤éš±è—å±¤\n",
    "    return outputs.hidden_states[-1][0][0].numpy()\n",
    "\n",
    "emotions = [\"anticipation\", \"sadness\", \"fear\", \"joy\", \"anger\", \"trust\", \"disgust\", \"surprise\"]\n",
    "\n",
    "# è¨ˆç®—æ‰€æœ‰æƒ…æ„Ÿè©çš„embedding\n",
    "embeddings = np.array([get_embedding(emotion) for emotion in emotions])\n",
    "\n",
    "similarities = cosine_similarity(embeddings)\n",
    "similarity_sum = similarities.sum(axis=1)\n",
    "\n",
    "# æ ¹æ“šç›¸ä¼¼åº¦ç¸½å’Œé€²è¡Œæ’åº\n",
    "similarity_sum = [emotion for _, emotion in sorted(zip(similarity_sum, emotions), reverse=True)]\n",
    "\n",
    "sorted_indices = np.argsort(similarity_sum)  # å‡åº\n",
    "\n",
    "\n",
    "reordered_indices = []\n",
    "while len(sorted_indices) > 0:\n",
    "    if len(sorted_indices) % 2 == 0:\n",
    "        reordered_indices.append(sorted_indices[-1])  # å–æœ€é«˜çš„æ”¾ä¸­é–“\n",
    "        sorted_indices = sorted_indices[:-1]          # åˆªé™¤æœ€å¾Œä¸€å€‹å…ƒç´ \n",
    "    else:\n",
    "        reordered_indices.append(sorted_indices[0])   # å–æœ€ä½çš„æ”¾å…©ç«¯\n",
    "        sorted_indices = sorted_indices[1:]           # åˆªé™¤ç¬¬ä¸€å€‹å…ƒç´ \n",
    "        \n",
    "sorted_emotions = [emotions[i] for i in reordered_indices]\n",
    "sorted_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFåšwords embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.9, max_features=300,ngram_range=(1, 2)     )\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
    "train_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "                             \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(test_df[\"text\"])\n",
    "test_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "test_tfidf_df = test_tfidf_df.reindex(columns=train_tfidf_df.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ·»åŠ emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†trainæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90973/90973 [00:08<00:00, 10981.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†testæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25749/25749 [00:02<00:00, 11092.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è™•ç†å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ä¾†æå–è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\")\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# è™•ç†æ–‡æœ¬ä¸¦è¨ˆç®—æ¯æ¢æ–‡æœ¬çš„emojiæ•¸é‡\n",
    "def preprocess_and_add_to_df(texts, batch_size):\n",
    "    emoji_count_list = []  # ç”¨ä¾†å„²å­˜æ¯æ¢æ–‡æœ¬ä¸­çš„ emoji æ•¸é‡\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"è™•ç†ä¸­\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # è¨ˆç®—æ¯æ¢æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦è™Ÿæ•¸é‡\n",
    "        batch_emoji_counts = [len(extract_emojis(text)) for text in batch_texts]\n",
    "        emoji_count_list.extend(batch_emoji_counts)\n",
    "\n",
    "    return emoji_count_list\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†trainæ–‡æœ¬...\")\n",
    "train_emoji_counts = preprocess_and_add_to_df(\n",
    "    train_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "train_df[\"emoji_count\"] = train_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†testæ–‡æœ¬...\")\n",
    "test_emoji_counts = preprocess_and_add_to_df(\n",
    "    test_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "test_df[\"emoji_count\"] = test_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "print(\"è™•ç†å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆä½µTF-IDFå’Œemoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.25 GiB for an array with shape (300, 1455563) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# å°‡ emoji_count æ¬„ä½èˆ‡ TF-IDF ç‰¹å¾µåˆä½µ\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_tfidf_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memoji_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_tfidf_df, test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memoji_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# å‡è¨­ä½ çš„æ¨™ç±¤æ˜¯ \"emotion\" æ¬„ä½\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.25 GiB for an array with shape (300, 1455563) and data type float64"
     ]
    }
   ],
   "source": [
    "# å°‡ emoji_count æ¬„ä½èˆ‡ TF-IDF ç‰¹å¾µåˆä½µ\n",
    "train_features = pd.concat([train_tfidf_df, train_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "test_features = pd.concat([test_tfidf_df, test_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# å‡è¨­ä½ çš„æ¨™ç±¤æ˜¯ \"emotion\" æ¬„ä½\n",
    "train_labels = train_df[\"emotion\"].map(lambda x: emotion_to_id[x]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–éš¨æ©Ÿæ£®æ—æ¨¡å‹\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# é©—è­‰æ¨¡å‹\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "sample_df = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬\n",
    "test_predictions = rf_model.predict(test_features)\n",
    "\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "# ç²å–çœŸå¯¦æ¨™ç±¤å’Œé æ¸¬æ¨™ç±¤\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "# è¨ˆç®— mean-f1-score\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "# è¼¸å‡ºåˆ†æ•¸\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›æ­¸æ¨¡å‹\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬ä¸¦è¨ˆç®—å‡æ–¹èª¤å·®\n",
    "test_predictions = regressor.predict(test_features)\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMkaggle",
   "language": "python",
   "name": "dmkaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
