{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:æ¯›èƒ¤çµœ\n",
    "\n",
    "Student ID:113062596\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è¼‰è³‡æ–™ä¸¦åˆ†æˆtrainå’Œtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identification_df = pd.read_csv(\"data_identification.csv\")\n",
    "#all_data = pd.read_json(\"tweets_DM.json\", lines=True, encoding=\"utf-8\")\n",
    "#all_data_df = pd.DataFrame(all_data)\n",
    "#emotion_df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data_df['tweet_id'] = all_data_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "#all_data_df['hashtags'] = all_data_df['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "#all_data_df['text'] = all_data_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "#all_data_df = all_data_df.drop(columns=['_source'])\n",
    "\n",
    "#print(all_data_df['tweet_id'])\n",
    "\n",
    "# å°‡all_data_dfå’Œidentification_dfæ ¹æ“štweet_idåšåˆä½µ\n",
    "#merged_df = pd.merge(all_data_df, identification_df, on='tweet_id', how='inner')\n",
    "\n",
    "# åˆ†é›¢å‡ºtrain_dfå’Œtest_df\n",
    "#train_df = pd.merge(merged_df[merged_df['identification']=='train'], emotion_df, on='tweet_id', how='inner')\n",
    "#train_df = train_df.drop(columns=['identification','_type','_index'])\n",
    "\n",
    "\n",
    "\n",
    "#test_df = merged_df[merged_df['identification'] == 'test']\n",
    "#test_df = test_df.drop(columns=['identification','_type','_index']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è™•ç†è¡¨æƒ…ç¬¦è™Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(#ç”±GPTç”Ÿæˆ\n",
    "    \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# emojiæ¬„ä½å„²å­˜åœ¨textæ¬„ä½ä¸­æ‰¾åˆ°çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "train_df['emojis'] = train_df['text'].apply(lambda x: emoji_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì²˜', 'ğŸŒ¥', 'ğŸ˜¶', 'ï¾Ÿ', 'ã™', 'èª', 'ì•„', 'èƒ½', 'âš¡', 'âœ¯', 'ï·º', 'â™¡', 'ç‰‡', 'ìŠ¤', '\\uf648', 'ë¦„', 'ğŸ¤­', 'ğŸ¡', 'ãƒ¢', 'ğŸ…”', 'âš“', 'ğ—‚­', 'ë¬¸', 'è£½', 'ğŸŒ', 'å¿µ', 'ğŸ”', '\\ue00d', 'â•š', 'ğŸ¤ ']\n"
     ]
    }
   ],
   "source": [
    "# æŠŠæ‰€æœ‰è¡¨æƒ…ç¬¦è™Ÿåˆ—è¡¨å±•å¹³æˆä¸€å€‹å–®ä¸€çš„åˆ—è¡¨\n",
    "all_emojis = list(itertools.chain(*train_df['emojis']))\n",
    "\n",
    "# æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿçµ„åˆï¼Œä¸¦ä¿ç•™å”¯ä¸€çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = [emoji for sublist in all_emojis for emoji in sublist]  # æ‹†é–‹è¡¨æƒ…ç¬¦è™Ÿï¼Œå› ç‚ºå¯èƒ½é‚„ä¿æœ‰é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "unique_emojis = list(set(unique_emojis))  # å»é™¤é‡è¤‡çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "\n",
    "print(unique_emojis[0:30])# åŒ…å«å“ªäº›ç¬¦è™Ÿæˆ–æ˜¯éŸ“æ–‡ã€ä¸­æ–‡å­—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ†æè¡¨æƒ…ç¬¦è™Ÿè·Ÿemotionçš„é—œä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25508\\3835693287.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        emotion  unique_emoji_count\n",
      "0         anger                2269\n",
      "1  anticipation               14525\n",
      "2       disgust                5298\n",
      "3          fear                4358\n",
      "4           joy               37767\n",
      "5       sadness                7879\n",
      "6      surprise                3502\n",
      "7         trust               15586\n"
     ]
    }
   ],
   "source": [
    "# æ•´ç†å‡ºå…·æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„éƒ¨åˆ†\n",
    "train_df_with_emojis = train_df[train_df['emojis'].apply(len) > 0]\n",
    "train_df_with_emojis['emojis'] = train_df_with_emojis['emojis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# æ‹†é–‹æ‰€æœ‰ emoji(ç›¡é‡é¿å…é€£çºŒçš„emoji)\n",
    "all_emojis = list(itertools.chain(*train_df_with_emojis['emojis'].str.split()))\n",
    "\n",
    "# å»ºç«‹ emoji å’Œ emotion çš„çµ„åˆ\n",
    "emoji_emotion_data = []\n",
    "for index, row in train_df_with_emojis.iterrows():\n",
    "    emojis = row['emojis'].split()  # ä»¥ç©ºæ ¼åˆ†å‰²æ¯è¡Œä¸­çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emotion = row['emotion']\n",
    "    for emoji in emojis:\n",
    "        emoji_emotion_data.append([emoji, emotion])\n",
    "\n",
    "emoji_emotion_df = pd.DataFrame(emoji_emotion_data, columns=['emoji', 'emotion'])\n",
    "\n",
    "# çµ±è¨ˆæ¯å€‹emotionæœ‰å¤šå°‘å€‹ç¨ç‰¹çš„ emoji\n",
    "emotion_emoji_counts = emoji_emotion_df.groupby('emotion')['emoji'].nunique().reset_index(name='unique_emoji_count')\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(emotion_emoji_counts)# çœ‹å¾—å‡ºæ˜é¡¯å·®åˆ¥ï¼Œé›–ç„¶dfä¸­å¯èƒ½é‚„æ˜¯åŒ…å«é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿï¼Œä½†æ˜¯æŠŠå®ƒå€‘çœ‹åšä¸€å€‹è¡¨æƒ…ç¬¦è™Ÿé‚è¼¯ä¸Šä¹Ÿå¯ä»¥æ¥å—\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotion dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'fear': 1,\n",
       " 'disgust': 2,\n",
       " 'joy': 3,\n",
       " 'anticipation': 4,\n",
       " 'trust': 5,\n",
       " 'sadness': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_to_id = {'anger': 0,\n",
    " 'fear': 1,\n",
    " 'disgust': 2,\n",
    " 'joy': 3,\n",
    " 'anticipation': 4,\n",
    " 'trust': 5,\n",
    " 'sadness': 6,\n",
    " 'surprise': 7}\n",
    "id_to_emotion = {v: k for k, v in emotion_to_id.items()}\n",
    "emotion_to_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFåšwords embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.9, max_features=300,ngram_range=(1, 2)     )\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n",
    "train_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "                             \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(test_df[\"text\"])\n",
    "test_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "test_tfidf_df = test_tfidf_df.reindex(columns=train_tfidf_df.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ·»åŠ emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†trainæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90973/90973 [00:06<00:00, 15062.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è™•ç†testæ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25749/25749 [00:01<00:00, 13041.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è™•ç†å®Œæˆ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ä¾†æå–è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+\")\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# è™•ç†æ–‡æœ¬ä¸¦è¨ˆç®—æ¯æ¢æ–‡æœ¬çš„emojiæ•¸é‡\n",
    "def preprocess_and_add_to_df(texts, batch_size):\n",
    "    emoji_count_list = []  # ç”¨ä¾†å„²å­˜æ¯æ¢æ–‡æœ¬ä¸­çš„ emoji æ•¸é‡\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"è™•ç†ä¸­\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # è¨ˆç®—æ¯æ¢æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦è™Ÿæ•¸é‡\n",
    "        batch_emoji_counts = [len(extract_emojis(text)) for text in batch_texts]\n",
    "        emoji_count_list.extend(batch_emoji_counts)\n",
    "\n",
    "    return emoji_count_list\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†trainæ–‡æœ¬...\")\n",
    "train_emoji_counts = preprocess_and_add_to_df(\n",
    "    train_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "train_df[\"emoji_count\"] = train_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "# åªè™•ç† emoji_count\n",
    "print(\"æ­£åœ¨è™•ç†testæ–‡æœ¬...\")\n",
    "test_emoji_counts = preprocess_and_add_to_df(\n",
    "    test_df['text'].tolist(), batch_size=16\n",
    ")\n",
    "test_df[\"emoji_count\"] = test_emoji_counts  # æ·»åŠ emojiæ•¸é‡\n",
    "\n",
    "print(\"è™•ç†å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆä½µTF-IDFå’Œemoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡ emoji_count æ¬„ä½èˆ‡ TF-IDF ç‰¹å¾µåˆä½µ\n",
    "train_features = pd.concat([train_tfidf_df, train_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "test_features = pd.concat([test_tfidf_df, test_df[\"emoji_count\"].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# å‡è¨­ä½ çš„æ¨™ç±¤æ˜¯ \"emotion\" æ¬„ä½\n",
    "train_labels = train_df[\"emotion\"].map(lambda x: emotion_to_id[x]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4450627589191863\n"
     ]
    }
   ],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# åˆå§‹åŒ–éš¨æ©Ÿæ£®æ—æ¨¡å‹\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# é©—è­‰æ¨¡å‹\n",
    "val_predictions = rf_model.predict(X_val)\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-Score: 0.0019\n",
      "Accuracy: 0.0077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "sample_df = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "# ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬\n",
    "test_predictions = rf_model.predict(test_features)\n",
    "\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "# ç²å–çœŸå¯¦æ¨™ç±¤å’Œé æ¸¬æ¨™ç±¤\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission with Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.94 GiB for an array with shape (301, 1310006) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# å›æ­¸æ¨¡å‹\u001b[39;00m\n\u001b[0;32m      2\u001b[0m regressor \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# é æ¸¬ä¸¦è¨ˆç®—å‡æ–¹èª¤å·®\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mpredict(test_features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\sklearn\\utils\\_array_api.py:745\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    743\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\generic.py:2152\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m   2150\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2152\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\n\u001b[0;32m   2153\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2155\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2158\u001b[0m     ):\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\frame.py:1127\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[0;32m   1129\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12594\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DMkaggle\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.94 GiB for an array with shape (301, 1310006) and data type float64"
     ]
    }
   ],
   "source": [
    "# å›æ­¸æ¨¡å‹\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬ä¸¦è¨ˆç®—å‡æ–¹èª¤å·®\n",
    "test_predictions = regressor.predict(test_features)\n",
    "y_pred_labels = [id_to_emotion[pred] for pred in test_predictions]\n",
    "\n",
    "test_df['y_pred_labels'] = y_pred_labels\n",
    "test_df = test_df.rename(columns={'tweet_id': 'id'})\n",
    "merged_df = sample_df.merge(test_df[['id', 'y_pred_labels']], on='id', how='inner')\n",
    "\n",
    "true_labels = merged_df['emotion']\n",
    "pred_labels = merged_df['y_pred_labels']\n",
    "\n",
    "\n",
    "submission_mean_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"Mean F1-Score: {submission_mean_f1:.4f}\")\n",
    "submission_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {submission_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMkaggle",
   "language": "python",
   "name": "dmkaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
